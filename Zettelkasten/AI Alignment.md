20220402:1649
Tags: #effective_altruism #artificial_intelligence
Backlinks: [[Reward Modelling]] [[Artificial Intelligence]] [[Effective Altruism]] [[Existential Risk]] [[AI Alignment]]
# AI Alignment
The problem of aligning [[Artificial Intelligence]] to human values and a key consideration in [[AI Safety]] work. Considered by many knowledgeable people to be an [[Existential Risk]], one of the primary focuses of the [[Effective Altruism]] movement.

---
# References
[ğŸŒ Paul Christiano on Clarifying "AI alignment"](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)
[ğŸ“–The Alignment Problem by Brian Christian](https://brianchristian.org/the-alignment-problem/)
[ğŸŒ The AI Alignment Forum](https://www.alignmentforum.org/)